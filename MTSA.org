Multivarite Time Series Analysis
* R代码解释
** VAR 源代码
#+BEGIN_SRC  python
function (x, p = 1, output = T, include.mean = T, fixed = NULL) 
{
  #p是VAR(p)，意思是t时刻的数据与它前面p个时刻数据有关
  if (!is.matrix(x)) 
    x = as.matrix(x)
  Tn = dim(x)[1]    #---有多少行
  k = dim(x)[2]     #---有多少列，列数就是k表示k维数据
  if (p < 1)        
    p = 1
  idm = k * p
  ne = Tn - p       #一共有ne个数据可以计算（预测）
  ist = p + 1       #从ist个数据开始是可以预测的，因为ist前面没有p个数据
  y = x[ist:Tn, ]
  # 下面到27行，将元数据组织起来得到书上的X，每一行是[x(t),x(t-1)...x(t-p)]
  if (include.mean) {
    idm = idm + 1
    xmtx = cbind(rep(1, ne), x[p:(Tn - 1), ])
  }
  else {
    xmtx = x[p:(Tn - 1), ]
  }
  if (p > 1) {
    for (i in 2:p) {
      xmtx = cbind(xmtx, x[(ist - i):(Tn - i), ])   #一共是p*k列
    }
  }
  
  ndim = ncol(xmtx)                        #ndim表示xmtx有多少列
  if (length(fixed) == 0) {
    paridx = matrix(1, ndim, k)
  }
  else {
    paridx = fixed
  }
  res = NULL
  beta = matrix(0, ndim, k)
  sdbeta = matrix(0, ndim, k)
  npar = 0
  for (i in 1:k) {
    idx = c(1:ndim)[paridx[, i] == 1]       #返回的是paridx中第i列中数字是1数据的行敿
                                            #c(1:10)[c(11:20)>15] 的返回值是6 7 8 9 10
    resi = y[, i]
    if (length(idx) > 0) {
      xm = as.matrix(xmtx[, idx])           ### 默认下xm就是xmtx
      npar = npar + dim(xm)[2]
      xpx = t(xm) %*% xm                    #xpx是一个(p*k) * (p*k)矩阵
      xpxinv = solve(xpx)                   #求矩阵的逆
      xpy = t(xm) %*% as.matrix(y[, i], ne, 1)  #xpy是一个p*k * 1矩阵     【注】p*k是xmtx的列数 = ndim
      betai = xpxinv %*% xpy                #betai= (t(xm) . xm)^(-1) . t(xm) . t(y[,i]) 
      beta[idx, i] = betai                  #betai是一个p*k * 1
      resi = y[, i] - xm %*% betai          #resi ne*1
      #以上用到公式：P49定理2.2 vec(beta^) 所以说上面方法用的是ML估计 
      nee = dim(xm)[2]                      # nee = npar = ne
      sse = sum(resi * resi)/(Tn - p - nee) # 下面几行与模型检验有关,这行揭示残差协方差矩阵与残差（residual）之间的关系
      dd = diag(xpxinv)
      sdbeta[idx, i] = sqrt(dd * sse)
    }
    res = cbind(res, resi)
  }
  sse = t(res) %*% res/(Tn - p)             # 公式 2-55 求残差协方差矩阵
  aic = 0
  bic = 0
  hq = 0
  Phi = NULL
  Ph0 = NULL
  jst = 0
  if (include.mean) {
    Ph0 = beta[1, ]
    se = sdbeta[1, ]
    if (output) {
      cat("Constant term:", "\n")
      cat("Estimates: ", Ph0, "\n")
      cat("Std.Error: ", se, "\n")
    }
    jst = 1
  }
  if (include.mean) {
    for (i in 1:k) {
      if (abs(Ph0[i]) > 1e-08) 
        npar = npar - 1
    }
  }
  if (output) 
    cat("AR coefficient matrix", "\n")
  for (i in 1:p) {
    phi = t(beta[(jst + 1):(jst + k), ])
    se = t(sdbeta[(jst + 1):(jst + k), ])
    if (output) {
      cat("AR(", i, ")-matrix", "\n")
      print(phi, digits = 3)
      cat("standard error", "\n")
      print(se, digits = 3)
    }
    jst = jst + k
    Phi = cbind(Phi, phi)
  }
  if (output) {
    cat(" ", "\n")
    cat("Residuals cov-mtx:", "\n")
    print(sse)
    cat(" ", "\n")
  }
  dd = det(sse)
  d1 = log(dd)
  aic = d1 + (2 * npar)/Tn
  bic = d1 + log(Tn) * npar/Tn
  hq = d1 + 2 * log(log(Tn)) * npar/Tn
  if (output) {
    cat("det(SSE) = ", dd, "\n")
    cat("AIC = ", aic, "\n")
    cat("BIC = ", bic, "\n")
    cat("HQ  = ", hq, "\n")
  }
  VAR <- list(data = x, cnst = include.mean, order = p, coef = beta, 
              aic = aic, bic = bic, hq = hq, residuals = res, secoef = sdbeta, 
              Sigma = sse, Phi = Phi, Ph0 = Ph0)
}



#+END_SRC

** VARpred 函数源码
#+BEGIN_SRC python
function (model, h = 1, orig = 0, Out.level = F) 
{
  x = model$data
  Phi = model$Phi
  sig = model$Sigma    #Sigma是一个正定协方差矩阵，参考定理2.2里面的定义
  Ph0 = model$Ph0  
  p = model$order      #var(p)
  cnst = model$cnst    #ture
  np = dim(Phi)[2]
  k = dim(x)[2]
  nT = dim(x)[1]
  k = dim(x)[2]
  if (orig <= 0) 
    orig = nT
  if (orig > nT) 
    orig = nT
  psi = VARpsi(Phi, h)$psi 
  beta = t(Phi)
  if (length(Ph0) < 1) 
    Ph0 = rep(0, k)
  if (p > orig) {
    cat("Too few data points to produce forecasts", "\n")
  }
  pred = NULL
  se = NULL
  MSE = NULL
  mse = NULL
  px = as.matrix(x[1:orig, ])  #orig用于数据分割，只是取前orig行。
  Past = px[orig, ]
  if (p > 1) {
    for (j in 1:(p - 1)) {
      Past = c(Past, px[(orig - j), ])   #past是px的后p行展开成一维向量 即(px[-1],px[-2],...,px[-p])
    }
  }
  cat("orig ", orig, "\n")
  ne = orig - p
  xmtx = NULL
  P = NULL
  if (cnst) 
    xmtx = rep(1, ne)
  xmtx = cbind(xmtx, x[p:(orig - 1), ])
  ist = p + 1
  if (p > 1) {
    for (j in 2:p) {
      xmtx = cbind(xmtx, x[(ist - j):(orig - j), ])
    }
  }
  xmtx = as.matrix(xmtx)
  G = t(xmtx) %*% xmtx/ne
  Ginv = solve(G)
  P = Phi
  vv = Ph0
  if (p > 1) {
    II = diag(rep(1, k * (p - 1)))
    II = cbind(II, matrix(0, (p - 1) * k, k))
    P = rbind(P, II)
    vv = c(vv, rep(0, (p - 1) * k))
  }
  if (cnst) {
    c1 = c(1, rep(0, np))
    P = cbind(vv, P)
    P = rbind(c1, P)
  }
  Sig = sig
  n1 = dim(P)[2]
  MSE = (n1/orig) * sig
  for (j in 1:h) {
    tmp = Ph0 + matrix(Past, 1, np) %*% beta       # beta = t(Phi)
                                                   #tmp 就是预测h个值,如下面的注释，也就是说这函数就预测了h个值
    #QUESTION: 为什么这里没有加a这个向量？？ -->因为这是估计模型的预测
    px = rbind(px, tmp)
    #------------------预测部分到这结束，往下是预测结果的评价--------------------
    if (np > k) {
      Past = c(tmp, Past[1:(np - k)])
    }
    else {
      Past = tmp
    }
    if (j > 1) {
      idx = (j - 1) * k
      wk = psi[, (idx + 1):(idx + k)]
      Sig = Sig + wk %*% sig %*% t(wk)
    }
    if (j > 1) {
      for (ii in 0:(j - 1)) {
        psii = diag(rep(1, k))
        if (ii > 0) {
          idx = ii * k
          psii = psi[, (idx + 1):(idx + k)]
        }
        P1 = P^(j - 1 - ii) %*% Ginv
        for (jj in 0:(j - 1)) {
          psij = diag(rep(1, k))
          if (jj > 0) {
            jdx = jj * k
            psij = psi[, (jdx + 1):(jdx + k)]
          }
          P2 = P^(j - 1 - jj) %*% G
          k1 = sum(diag(P1 %*% P2))
          MSE = (k1/orig) * psii %*% sig %*% t(psij)
        }
      }
    }
    se = rbind(se, sqrt(diag(Sig)))
    if (Out.level) {
      cat("Covariance matrix of forecast errors at horizon: ", 
          j, "\n")
      print(Sig)
      cat("Omega matrix at horizon: ", j, "\n")
      print(MSE)
    }
    MSE = MSE + Sig
    mse = rbind(mse, sqrt(diag(MSE)))
  }
  cat("Forecasts at origin: ", orig, "\n")
  print(px[(orig + 1):(orig + h), ], digits = 4)
  cat("Standard Errors of predictions: ", "\n")
  print(se[1:h, ], digits = 4)
  pred = px[(orig + 1):(orig + h), ]
  cat("Root mean square errors of predictions: ", "\n")
  print(mse[1:h, ], digits = 4)
  if (orig < nT) {
    cat("Observations, predicted values,     errors, and MSE", 
        "\n")
    tmp = NULL
    jend = min(nT, (orig + h))
    for (t in (orig + 1):jend) {
      case = c(t, x[t, ], px[t, ], x[t, ] - px[t, ])
      tmp = rbind(tmp, case)
    }
    colnames(tmp) <- c("time", rep("obs", k), rep("fcst", 
                                                  k), rep("err", k))
    idx = c(1)
    for (j in 1:k) {
      idx = c(idx, c(0, 1, 2) * k + j + 1)
    }
    tmp = tmp[, idx]
    print(round(tmp, 4))
  }
  VARpred <- list(pred = pred, se.err = se, mse = mse)
}

#+END_SRC

** VMA 函数源码
#+BEGIN_SRC python
function (da, q = 1, include.mean = T, fixed = NULL, beta = NULL, 
          sebeta = NULL, prelim = F, details = F, thres = 2) 
{
  if (!is.matrix(da)) 
    da = as.matrix(da)
  nT = dim(da)[1]
  k = dim(da)[2]
  if (q < 1) 
    q = 1
  kq = k * q
  #P94页3.3.3 初始参数估计一节表明 VMA的参数的初始值定义为μ_0_hat = c_hat ; θ_i_hat = -β_i_hat
  # 其中c_hat和β_i_hat是用LS来拟合VAR模型的估计结果。这也就是THini函数出现的原因。可以看到他其
  # 实就是把VAR源码里的估计的部分拿了出来
  THini <- function(y, x, q, include.mean) {
    if (!is.matrix(y)) 
      y = as.matrix(y)
    if (!is.matrix(x)) 
      x = as.matrix(x)
    nT = dim(y)[1]   # 610
    k = dim(y)[2]    # 2
    ist = 1 + q
    ne = nT - q
    if (include.mean) {
      xmtx = matrix(1, ne, 1)
    }
    else {
      xmtx = NULL
    }
    ymtx = y[ist:nT, ]
    for (j in 1:q) {
      xmtx = cbind(xmtx, x[(ist - j):(nT - j), ])
    }                                                               ## xmtx: ne*(kq+1)
                                                                    ## ymtx: ne*k
    xtx = crossprod(xmtx, xmtx)  #crossprod(x,y)相当于t(x) %*% y    ## xtx : (kq+1)*(kq+1)
    xty = crossprod(xmtx, ymtx)                                     ## xty : (kq+1)*k
    xtxinv = solve(xtx)         #xtx的逆
    beta = xtxinv %*% xty                                           ## beta: (kq+1)*k
    resi = ymtx - xmtx %*% beta 
    sse = crossprod(resi, resi)/ne
    dd = diag(xtxinv)
    sebeta = NULL
    for (j in 1:k) {
      se = sqrt(dd * sse[j, j])
      sebeta = cbind(sebeta, se)
    }
    THini <- list(estimates = beta, se = sebeta)
  }
  if (length(fixed) < 1) {
    m1 = VARorder(da, q + 12, output = FALSE)  # VARorfer  VAR模型的阶选择函数
    porder = m1$aicor                          # m1$aicor得到的是通过aic这个
    if (porder < 1) 
      porder = 1
    m2 = VAR(da, porder, output = FALSE)
    y = da[(porder + 1):nT, ]
    x = m2$residuals                           #residuals：残差
    m3 = THini(y, x, q, include.mean)          #函数VAR()中有:resi = y[, i] - xm %*% betai // 在这里残差就是正确值与预测值的差
    # 残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。“残差”蕴含了有关模型基本假设的重要信息。如果回归模型正确的话， 我们可以将残差看作误差的观测值。
    beta = m3$estimates
    sebeta = m3$se
    nr = dim(beta)[1]
    if (prelim) {
      fixed = matrix(0, nr, k)
      for (j in 1:k) {
        tt = beta[, j]/sebeta[, j]
        idx = c(1:nr)[abs(tt) >= thres]
        fixed[idx, j] = 1
      }
    }
    if (length(fixed) < 1) {
      fixed = matrix(1, nr, k)
    }
  }
  else {
    nr = dim(beta)[1]
  }
  par = NULL
  separ = NULL
  fix1 = fixed
  VMAcnt = 0
  ist = 0
  if (include.mean) {
    jdx = c(1:k)[fix1[1, ] == 1]   #fixed中第一行中值是1的列数
    VMAcnt = length(jdx)
    if (VMAcnt > 0) {
      par = beta[1, jdx]
      separ = sebeta[1, jdx]
    }
    TH = -beta[2:(kq + 1), ]
    seTH = sebeta[2:(kq + 1), ]
    ist = 1
  }
  else {
    TH = -beta
    seTH = sebeta
  }
  for (j in 1:k) {
    idx = c(1:(nr - ist))[fix1[(ist + 1):nr, j] == 1]
    if (length(idx) > 0) {
      par = c(par, TH[idx, j])                  # 当fix为默认情况的时候，par=TH separ=seTH
      separ = c(separ, seTH[idx, j])
    }
  }
  ParMA <- par                                  # 当fix为默认情况的时候，ParMA=par=TH 
  LLKvma <- function(par, zt = zt, q = q, fixed = fix1, include.mean = include.mean) {
    # 最小似然估计的输入函数
    k = ncol(zt)
    nT = nrow(zt)
    mu = rep(0, k)
    icnt = 0
    VMAcnt <- 0
    fix <- fixed
    iist = 0
    if (include.mean) {
      iist = 1
      jdx = c(1:k)[fix[1, ] == 1]
      icnt = length(jdx)
      VMAcnt <- icnt
      if (icnt > 0) 
        mu[jdx] = par[1:icnt]
    }
    for (j in 1:k) {
      zt[, j] = zt[, j] - mu[j]
    }
    kq = k * q
    Theta = matrix(0, kq, k)
    for (j in 1:k) {
      idx = c(1:kq)[fix[(iist + 1):(iist + kq), j] == 
                      1]
      jcnt = length(idx)
      if (jcnt > 0) {
        Theta[idx, j] = par[(icnt + 1):(icnt + jcnt)]
        icnt = icnt + jcnt
      }
    }
    TH = t(Theta)
    if (q > 1) {
      tmp = cbind(diag(rep(1, (q - 1) * k)), matrix(0, 
                                                    (q - 1) * k, k))
      TH = rbind(TH, tmp)
    }
    mm = eigen(TH)   #特征分解（Eigendecomposition），又称谱分解（Spectral decomposition）是将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法。
                     #需要注意只有对可对角化矩阵才可以施以特征分解。
    V1 = mm$values
    P1 = mm$vectors
    v1 = Mod(V1)     #复数的函数，如果V1 = x + yi ，那么Mod(V1) = sqrt(x^2 + y^2) .即求负数V1的模
    ich = 0
    for (i in 1:kq) {
      if (v1[i] > 1) 
        V1[i] = 1/V1[i]
      ich = 1
    }
    if (ich > 0) {
      P1i = solve(P1)              #solve(A) A矩阵求逆
      GG = diag(V1)
      TH = Re(P1 %*% GG %*% P1i)   #Re()函数 只取负数的real部分，保留原来格式。同理Im()函数只取虚数部分
      Theta = t(TH[1:k, ])
      ist = 0
      if (VMAcnt > 0) 
        ist = 1
      for (j in 1:k) {
        idx = c(1:kq)[fix[(ist + 1):(ist + kq), j] == 
                        1]
        jcnt = length(idx)
        if (jcnt > 0) {
          par[(icnt + 1):(icnt + jcnt)] = TH[j, idx]
          icnt = icnt + jcnt
        }
      }
    }
    at = mFilter(zt, t(Theta))
    sig = t(at) %*% at/nT
    ll = dmvnorm(at, rep(0, k), sig)  #多元正态密度和随机偏差，这句的意思是？？？-->
                                      #在一个均值是rep(0,k)方差是sig的多元正太分布中取到at[i]的概率分别是多少
    LLKvma = -sum(log(ll))
    LLKvma
  }
  scat("Number of parameter s: ", length(par), "\n")
  cat("initial estimates: " , round(par, 4), "\n")
  lowerBounds = par
  upperBounds = par
  npar = length(par)
  mult = 2
  if ((npar > 10) || (q > 2)) 
    mult = 1.2
  for (j in 1:npar) {
    lowerBounds[j] = par[j] - mult * separ[j]
    upperBounds[j] = par[j] + mult * separ[j]
  }
  cat("Par. Lower-bounds: ", round(lowerBounds, 4), "\n")
  cat("Par. Upper-bounds: ", round(upperBounds, 4), "\n")
  #---
  #一共有length(ParMA)个参数，初始值是parMA
  #需要最大似然概率函数是LLKvma函数，前面有定义
  if (details) {
    fit = nlminb(start = ParMA, objective = LLKvma, zt = da, 
                 fixed = fixed, include.mean = include.mean, q = q, 
                 lower = lowerBounds, upper = upperBounds, control = list(trace = 3))
  }
  else {
    fit = nlminb(start = ParMA, objective = LLKvma, zt = da, 
                 fixed = fixed, include.mean = include.mean, q = q, 
                 lower = lowerBounds, upper = upperBounds)
  }
  epsilon = 1e-04 * fit$par
  npar = length(par)
  Hessian = matrix(0, ncol = npar, nrow = npar)
  for (i in 1:npar) {
    for (j in 1:npar) {
      x1 = x2 = x3 = x4 = fit$par
      x1[i] = x1[i] + epsilon[i]
      x1[j] = x1[j] + epsilon[j]
      x2[i] = x2[i] + epsilon[i]
      x2[j] = x2[j] - epsilon[j]
      x3[i] = x3[i] - epsilon[i]
      x3[j] = x3[j] + epsilon[j]
      x4[i] = x4[i] - epsilon[i]
      x4[j] = x4[j] - epsilon[j]
      Hessian[i, j] = (LLKvma(x1, zt = da, q = q, fixed = fixed, 
                              include.mean = include.mean) - LLKvma(x2, zt = da, 
                                                                    q = q, fixed = fixed, include.mean = include.mean) - 
                         LLKvma(x3, zt = da, q = q, fixed = fixed, include.mean = include.mean) + 
                         LLKvma(x4, zt = da, q = q, fixed = fixed, include.mean = include.mean))/(4 * 
                                                                                                    epsilon[i] * epsilon[j])
    }
  }
  est = fit$par
  cat("Final   Estimates: ", est, "\n")
  se.coef = sqrt(diag(solve(Hessian)))
  tval = fit$par/se.coef
  matcoef = cbind(fit$par, se.coef, tval, 2 * (1 - pnorm(abs(tval))))
  dimnames(matcoef) = list(names(tval), c(" Estimate", " Std. Error", 
                                          " t value", "Pr(>|t|)"))
  cat("\nCoefficient(s):\n")
  printCoefmat(matcoef, digits = 4, signif.stars = TRUE)
  cat("---", "\n")
  cat("Estimates in matrix form:", "\n")
  icnt = 0
  ist = 0
  cnt = NULL
  if (include.mean) {
    ist = 1
    cnt = rep(0, k)
    secnt = rep(1, k)
    jdx = c(1:k)[fix1[1, ] == 1]
    icnt = length(jdx)
    if (icnt > 0) {
      cnt[jdx] = est[1:icnt]
      secnt[jdx] = se.coef[1:icnt]
      cat("Constant term: ", "\n")
      cat("Estimates: ", cnt, "\n")
    }
  }
  cat("MA coefficient matrix", "\n")
  TH = matrix(0, kq, k)
  seTH = matrix(1, kq, k)
  for (j in 1:k) {
    idx = c(1:kq)[fix1[(ist + 1):nr, j] == 1]
    jcnt = length(idx)
    if (jcnt > 0) {
      TH[idx, j] = est[(icnt + 1):(icnt + jcnt)]
      seTH[idx, j] = se.coef[(icnt + 1):(icnt + jcnt)]
      icnt = icnt + jcnt
    }
  }
  icnt = 0
  for (i in 1:q) {
    cat("MA(", i, ")-matrix", "\n")
    theta = t(TH[(icnt + 1):(icnt + k), ])
    print(theta, digits = 3)
    icnt = icnt + k
  }
  zt = da
  if (include.mean) {
    for (i in 1:k) {
      zt[, i] = zt[, i] - cnt[i]
    }
  }
  at = mFilter(zt, t(TH))
  sig = t(at) %*% at/nT
  cat(" ", "\n")
  cat("Residuals cov-matrix:", "\n")
  print(sig)
  dd = det(sig)
  d1 = log(dd)
  aic = d1 + 2 * npar/nT
  bic = d1 + log(nT) * npar/nT
  cat("----", "\n")
  cat("aic= ", aic, "\n")
  cat("bic= ", bic, "\n")
  Theta = t(TH)
  if (include.mean) {
    TH = rbind(cnt, TH)
    seTH = rbind(secnt, seTH)
  }
  VMA <- list(data = da, MAorder = q, cnst = include.mean, 
              coef = TH, secoef = seTH, residuals = at, Sigma = sig, 
              Theta = Theta, mu = cnt, aic = aic, bic = bic)
}
#+END_SRC


** VMA函数

VMA模型主要用到了书上的P85页 3.7 下面的函数。也用到了3.7上面的一列，我们要估计的是θ （维度是1*k）。

要最小化的最大似然函数和nlminb函数定义是这样的：

LLKvma <- function(par, zt = zt, q = q, fixed = fix1, include.mean = include.mean) 

fit = nlminb(start = ParMA, objective = LLKvma, zt = da, 
                 fixed = fixed, include.mean = include.mean, q = q, 
                 lower = lowerBounds, upper = upperBounds, control = list(trace = 3))

*参数* ：start是我们给定的估计参数的初始值，objective是最大使然函数里的那个我们要最小化他的返回值的那个函数，zt是实际数据da，q是VMA(q)中的那个q。

其中，par是我们要估计的参数，即μ和θ_i,一共是(kq+1)*k个变量（μ是1*k，θ_i是1*k,一共q个θ）。
-sum(log(dmvonorm(at,rep(0,k),sig)) == 书中的Σlog(P(a_t|θ1，Σa)) 

* 概念知识
** 弱平稳
弱平稳就是说Zt的均值和协方差矩阵不依赖于时间t，即zt的前两阶矩不随时间变化。（一个隐函条件是这两个统计变量都是存在的）

** 线性
z_t = μ + Σ_(0-)

** 交叉相关矩阵
   + 滞后为l的 *交叉协方差矩阵* ：τ_l=Cov(z_t,z_(t-l)) 
   + 滞后为l的 *交叉相关矩阵* :CCMρ_l = D^-1 τ_1 D^-1

这两个变量的估计方值在1.4有介绍

** 多元混成检验
Ljung-Box检验统计量Q_k(m)

** 自回归模型（AR）、移动平均模型（MA）、自回归移动平均模型（ARMA）以及差分自回归移动平均模型（ARIMA）辨析  
   + [[http://lidequan12345.blog.163.com/blog/static/28985036201321074325444/][这个博客]]
   + 
** 不存在封闭解
   是指有解但是我们就是就是求不出来，可以利用数值方法估计一下。
** 残差
  残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。“残差”蕴含了有关模型基本假设的重要信息。

  如果回归模型正确的话， 我们可以将残差看作误差的观测值。
* R语言函数的一些注释
** diff(x,lag,differences)
diff是返回滞后的迭代差异。

意思就是dif(1:10,2)的话就是所有数减去他的前2个数的值，所以是（2,2,2,2,2,2,2,2）一共8个数，因为1和2没有滞后2个的数
同理diff(1:10,6) = (6,6,6,6)

参数：
  + x： 数据
  + lag：滞后的步数
  + differences：没用到所以暂时不研究

** rmvnorm
   rmvnorm函数生成一个正态分布的数据，其中300是向量矩阵，rep(0.2)=(0,0),表示两列向量的均值都是0，di
** Mod(V)  
复数的函数，如果V1 = x + yi ，那么Mod(V1) = sqrt(x^2 + y^2) .即求负数V1的模
** solve(A,B) 
这个函数是求解 Ax=B的x，即 x = B%*%A^-1。

如果只是solve(A)的话，那么就是求A矩阵的逆
** nlminb 函数   
可以用来计算极大似然函数
[[https://cosx.org/2009/07/maximum-likelihood-estimation-in-r/][参考网址]]  上是使用实例讲的很清楚
** dnorm(x,mean,sd)
生成的是在N(mean,sd)的正态分布中取x的概率，如果x是个向量的话那么就是每个数的概率，

反正length(x) = length(dnorm(x,mean,sd))

_______________________________________________________________________________________
* QUESTIONS
1. 方程的phi是怎么确定的？？在例子程序中，都是直接给出来的。那么在实际中呢？？？

   解：是估计出来的，在模型估计中有公式
2. VAR模型中的Sigmaa是什么用途的？？

   解：Sigma是残差协方差矩阵，是用来进行模型估计的。在VAR(p)模型的估计过程，主要估计的就是Phi和Sigma。
3. include.mean 的意义是什么？？
4. 在varpred函数里面预测时候并没有加a这个向量？

   解：在P2.9.2 估计模型的预测中，可以看到估计模型的预测是不需要加最后的a向量的。只是这样： z_h = φ0 + Σφ_i z_h(l-i)
       就可以了

* 改成C语言的可能障碍
** DONE eigen函数 是矩阵的谱分解函数，不知道C语言里面有没有相应的函数来解决矩阵的谱分解
** DONE solve函数来求矩阵的逆
** TODO 还有一个nlminb函数是一个优化函数，不知道具体是怎么实现的。可能还需要在深入看这个函数的源代码
nlminb(start, objective, gradient = NULL, hessian = NULL, ...,
       scale = 1, control = list(), lower = -Inf, upper = Inf)

一些参数：
+ objective                   <-看这个参数的解释是这个方法是要最小化一个方法？？	
 Function to be minimized（要最小化的方法）. Must return a scalar value. The first argument to objective 
 is the vector of parameters to be optimized, whose initial values are supplied through start.
 Further arguments (fixed during the course of the optimization) to objective may be specified as well
 (see ...).
+ start	
numeric vector, initial values for the parameters to be optimized.



* 一些函数的c语言版本：
** eigen函数
#+BEGIN_SRC c -
  //------------------------------------头文件---------------------------------  
  #include <stdio.h>
  #include <stdlib.h>
  #include <math.h>
  #include <time.h>
  //--------------------------这里是一些定义的结构体和数据类型---------  
  //纯C里面定义的布尔类型  
  typedef enum{False = 0,True = 1}Bool;  
  //定义矩阵元素的类型为matrixType  
  typedef double matrixType;  
   
  //此结构体用来表示矩阵，其中row为行，column为列，height为高，array用来存放矩阵元素(用一维来模拟二维/三维)  
  typedef struct  
  {  
    unsigned  row,column,height;  
    matrixType *array; //使用时，必须对*array进行初始化  
  }Matrix;  
   
  //---------下面是QR分解，求解线性方程所用到的一些函数-----------  
  /* 
     matrix为要设置大小并分配内存的矩阵，row、column、height分别为行，列，高。 
     函数调用成功则则返回true,否则返回false 
  ,*/  
  Bool SetMatrixSize(Matrix *matrix ,const unsigned row,const unsigned column,const unsigned height)  
  {  
    unsigned size = row  * column * height * sizeof(matrixType);  
    if(size <= 0 )  
      {  
        return False;  
      }  
     
    matrix->array = (matrixType*)malloc(size);  
   
    //如果分配内存成功  
    if(matrix->array)  
      {  
        matrix->row = row;  
        matrix->column = column;  
        matrix->height = height;  
        return True;  
      }  
    else  
      {  
        matrix->row = matrix->column = matrix->height = 0;  
        return False;  
      }  
  }  
   
  //设置Matrix矩阵中的所有元素大小为ele  
  void SetMatrixEle(Matrix *matrix,matrixType ele)  
  {  
    unsigned size = matrix->row * matrix->column * matrix->height;  
    unsigned i;  
   
    for(i = 0;i < size;++i)  
      {  
        matrix->array[i] = ele;  
      }  
  }  
   
  //设置Matrix矩阵中的所有元素大小为0  
  void SetMatrixZero(Matrix*matrix)  
  {  
    SetMatrixEle(matrix,0);  
  }  
   
  //判断矩阵是否为空，若为空则返回1，否则返回0  
  Bool IsNullMatrix(const Matrix* matrix)  
  {  
    unsigned size = matrix->row * matrix->column * matrix->column;  
   
    if(size <= 0 || matrix->array == NULL)  
      {  
        return True;  
      }  
    return False;  
  }  
   
  //销毁矩阵，即释放为矩阵动态分配的内存,并将矩阵的长宽高置0  
  void DestroyMatrix(Matrix *matrix)  
  {  
    if(!IsNullMatrix(matrix))  
      {  
        free(matrix->array);  
        matrix->array = NULL;  
      }  
   
    matrix->row = matrix->column = matrix->height = 0;  
  }  
   
  //计算矩阵可容纳元素个数，即return row*column*height  
  unsigned MatrixCapacity(const Matrix*matrix)  
  {  
    return matrix->row * matrix->column * matrix->height;  
  }  
   
   
  //||matrix||_2  求A矩阵的2范数  
  matrixType MatrixNorm2(const Matrix *matrix)  
  {  
    unsigned size = matrix->row * matrix->column *matrix->height;  
    unsigned i;  
    matrixType norm = 0;  
   
    for(i = 0;i < size;++i)  
      {  
        norm +=  (matrix->array[i]) *(matrix->array[i]);  
      }  
   
    return (matrixType)sqrt(norm);  
  }  
   
  //matrixB = matrix(:,:,height)即拷贝三维矩阵的某层，若matrix为二维矩阵，需将height设置为0  
  Bool CopyMatrix(Matrix* matrixB,Matrix *matrix,unsigned height)  
  {  
    unsigned size,i;  
    Matrix matrixA;  
   
    //判断height值是否正确  
    if(height < 0 || height >= matrix->height)  
      {  
        printf("ERROR: CopyMatrix() parameter error！\n");  
        return False;  
      }  
   
    //将matrix(:,:,height) 转换为二维矩阵matrixA  
    matrixA.row = matrix->row;  
    matrixA.column = matrix->column;  
    matrixA.height = 1;  
    matrixA.array = matrix->array + height * matrix->row * matrix->column;  
   
    //判断两矩阵指向的内存是否相等  
    if(matrixA.array == matrixB->array)  
      {  
        return True;  
      }  
   
    //计算matrixA的容量  
    size = MatrixCapacity(&matrixA);  
    //判断matrixB的容量与matrixA的容量是否相等  
    if( MatrixCapacity(matrixB)!= size)  
      {  
        DestroyMatrix(matrixB);  
        SetMatrixSize(matrixB,matrixA.row,matrixA.column,matrixA.height);  
      }  
    else  
      {  
        matrixB->row = matrixA.row;  
        matrixB->column = matrixA.column;  
        matrixB->height = matrixA.height;  
      }  
   
    for(i = 0;i < size;++i)  
      {  
        matrixB->array[i] = matrixA.array[i];  
      }  
   
    return True;  
  }  
   
  //matrixC = matrixA * matrixB  
  Bool MatrixMulMatrix(Matrix *matrixC,const Matrix* matrixA,const Matrix* matrixB)  
  {  
    size_t row_i,column_i,i;  
    size_t indexA,indexB,indexC;  
    matrixType temp;  
    Matrix tempC;  
   
    if(IsNullMatrix(matrixA) || IsNullMatrix(matrixB))  
      {  
        return False;  
      }  
   
    if(matrixA->column != matrixB->row  )  
      {  
        return False;  
      }  
   
    if(MatrixCapacity(matrixC) != matrixA->row * matrixB->column)  
      {  
        SetMatrixSize(&tempC,matrixA->row,matrixB->column,1);  
      }  
    else  
      {  
        tempC.array = matrixC->array;  
        tempC.row = matrixA->row;  
        tempC.column = matrixB->column;  
        tempC.height = 1;  
      }  
   
    for(row_i = 0;row_i < tempC.row;++row_i)  
      {  
        for(column_i = 0;column_i < tempC.column;++column_i)  
          {  
            temp = 0;  
            for(i = 0;i < matrixA->column;++i)  
              {  
                indexA =  row_i * matrixA->column + i;  
                indexB =  i * matrixB->column + column_i;  
   
                temp += matrixA->array[indexA] * matrixB->array[indexB];  
              }  
            indexC = row_i * tempC.column + column_i;  
   
            tempC.array[indexC] = temp;  
          }  
      }  
   
   
    if(tempC.array != matrixC->array)  
      {  
        DestroyMatrix(matrixC);  
   
        matrixC->array = tempC.array;  
      }  
   
    matrixC->row = tempC.row;  
    matrixC->column = tempC.column;  
    matrixC->height = tempC.height;  
   
   
   
    return True;  
  }  
   
  //对vector中所有元素排序，sign= 0 时为升序，其余为降序  
  void SortVector(Matrix *vector,int sign)  
  {  
    matrixType mid;  
    int midIndex;  
    int size = MatrixCapacity(vector);  
    int i,j;  
   
    if(0 == sign)  
      {  
        for(i = 0;i < size;++i)  
          {  
            mid = vector->array[i];  
            midIndex = i;  
            for( j = i + 1; j < size ; ++j)  
              {  
                if(mid > vector->array[j])  
                  {  
                    mid = vector->array[j];  
                    midIndex = j;  
                  }  
              }  
   
            vector->array[midIndex] = vector->array[i];  
            vector->array[i] = mid;  
          }  
      }  
    else  
      {  
        for(i = 0;i < size;++i)  
          {  
            mid = vector->array[i];  
            midIndex = i;  
            for( j = i + 1; j < size ; ++j)  
              {  
                if(mid < vector->array[j])  
                  {  
                    mid = vector->array[j];  
                    midIndex = j;  
                  }  
              }  
   
            vector->array[midIndex] = vector->array[i];  
            vector->array[i] = mid;  
          }  
      }  
  }  
   
  //打印矩阵  
  void PrintMatrix(const Matrix *matrix)  
  {  
    size_t row_i,column_i,height_i,index;  
   
    for(height_i = 0;height_i < matrix->height;++height_i)  
      {  
        (matrix->height == 1) ? printf("[:,:] = \n"):printf("[%d,:,:] = \n",height_i);  
   
        for(row_i = 0;row_i < matrix->row;++row_i)  
          {  
            for(column_i = 0;column_i < matrix->column;++column_i)  
              {  
                index = height_i * matrix->row * matrix->column + row_i * matrix->column + column_i;  
                printf("%12.4g",matrix->array[index]);  
              }  
            printf("\n");  
          }  
      }  
  }  
   
  //----------------------QR分解-------------------------------------------  
   
  //将A分解为Q和R  
  void QR(Matrix *A,Matrix *Q,Matrix *R)  
  {  
    unsigned  i,j,k,m;  
    unsigned size;  
    const unsigned N = A->row;  
    matrixType temp;  
   
    Matrix a,b;  
   
    //如果A不是一个二维方阵，则提示错误，函数计算结束  
    if(A->row != A->column || 1 != A->height)  
      {  
        printf("ERROE: QR() parameter A is not a square matrix!\n");  
        return;  
      }  
   
    size = MatrixCapacity(A);  
    if(MatrixCapacity(Q) != size)  
      {  
        DestroyMatrix(Q);  
        SetMatrixSize(Q,A->row,A->column,A->height);  
        SetMatrixZero(Q);  
      }  
    else  
      {  
        Q->row = A->row;  
        Q->column = A->column;  
        Q->height = A->height;  
      }  
   
    if(MatrixCapacity(R)  != size)  
      {  
        DestroyMatrix(R);  
        SetMatrixSize(R,A->row,A->column,A->height);  
        SetMatrixZero(R);  
      }  
    else  
      {  
        R->row = A->row;  
        R->column = A->column;  
        R->height = A->height;  
      }  
   
    SetMatrixSize(&a,N,1,1);  
    SetMatrixSize(&b,N,1,1);  
   
    for(j = 0 ; j < N;++j)  
      {  
        for(i = 0;i < N; ++i)  
          {  
            a.array[i] = b.array[i] = A->array[i * A->column + j];  
          }  
   
        for(k  = 0; k  < j; ++k)  
          {  
            R->array[k * R->column + j] = 0;  
   
            for(m = 0;m < N; ++m)  
              {  
                R->array[k * R->column + j] += a.array[m] * Q->array[m * Q->column + k];  
              }  
   
            for(m = 0;m < N; ++m)  
              {  
                b.array[m] -= R->array[k * R->column + j] * Q->array[m * Q->column + k];  
              }  
          }  
   
        temp = MatrixNorm2(&b);  
        R->array[j * R->column + j] = temp;  
   
        for(i = 0; i < N; ++i)  
          {  
            Q->array[i * Q->column + j] = b.array[i] / temp;  
          }  
      }  
   
    DestroyMatrix(&a);  
    DestroyMatrix(&b);  
  }  
   
  //----------------------使用特征值计算矩阵特征向量-----------------  
  //eigenVector为计算结果即矩阵A的特征向量  
  //eigenValue为矩阵A的所有特征值，  
  //A为要计算特征向量的矩阵  
  void Eigenvectors(Matrix *eigenVector, Matrix *A,Matrix *eigenValue)  
  {  
    unsigned i,j,q;  
    unsigned count;  
    int m;  
    const unsigned NUM = A->column;  
    matrixType eValue;  
    matrixType sum,midSum,mid;  
    Matrix temp;  
   
    SetMatrixSize(&temp,A->row,A->column,A->height);  
   
    for(count = 0; count < NUM;++count)  
      {  
        //计算特征值为eValue，求解特征向量时的系数矩阵  
        eValue = eigenValue->array[count] ;  
        CopyMatrix(&temp,A,0);  
        for(i = 0 ; i < temp.column ; ++i)  
          {  
            temp.array[i * temp.column + i] -= eValue;  
          }  
   
        //将temp化为阶梯型矩阵  
        for(i = 0 ; i < temp.row - 1 ; ++i)  
          {  
            mid = temp.array[i * temp.column + i];  
            for(j = i; j < temp.column; ++j)  
              {  
                temp.array[i * temp.column + j] /= mid;  
              }  
   
            for(j = i + 1;j < temp.row;++j)  
              {  
                mid = temp.array[j * temp.column + i];  
                for(q = i ; q < temp.column; ++q)  
                  {  
                    temp.array[j * temp.column + q] -= mid * temp.array[i * temp.column + q];  
                  }  
              }  
          }  
        midSum = eigenVector->array[(eigenVector->row - 1) * eigenVector->column + count] = 1;  
        for(m = temp.row - 2; m >= 0; --m)  
          {  
            sum = 0;  
            for(j = m + 1;j < temp.column; ++j)  
              {  
                sum += temp.array[m * temp.column + j] * eigenVector->array[j * eigenVector->column + count];  
              }  
            sum= -sum / temp.array[m * temp.column + m];  
            midSum +=  sum * sum;  
            eigenVector->array[m * eigenVector->column + count] = sum;  
          }  
   
        midSum = sqrt(midSum);  
        for(i = 0; i < eigenVector->row ; ++i)  
          {  
            eigenVector->array[i * eigenVector->column + count] /= midSum;  
          }  
      }  
    DestroyMatrix(&temp);  
  }  
  int main()  
  {  
    const unsigned NUM = 50; //最大迭代次数  
   
    unsigned N = 3;  
    unsigned k;  
   
    Matrix A,Q,R,temp;  
    Matrix eValue;  
   
   
    //分配内存  
    SetMatrixSize(&A,N,N,1);  
    SetMatrixSize(&Q,A.row,A.column,A.height);  
    SetMatrixSize(&R,A.row,A.column,A.height);  
    SetMatrixSize(&temp,A.row,A.column,A.height);  
    SetMatrixSize(&eValue,A.row,1,1);  
   
    //A设置为一个简单矩阵  
    A.array[0] = -1;  
    A.array[1] = 2;  
    A.array[2] = 1;  
    A.array[3] = 2;  
    A.array[4] = 4;  
    A.array[5] = -1;  
    A.array[6] = 1;  
    A.array[7] = 1;  
    A.array[8] = -6;  
   
   
    //拷贝A矩阵元素至temp  
    CopyMatrix(&temp,&A,0);  
   
    //初始化Q、R所有元素为0  
    SetMatrixZero(&Q);  
    SetMatrixZero(&R);  
    //使用QR分解求矩阵特征值  
    for(k = 0;k < NUM; ++k)  
      {  
        QR(&temp,&Q,&R);  
        MatrixMulMatrix(&temp,&R,&Q);  
      }  
    //获取特征值，将之存储于eValue  
    for(k = 0;k < temp.column;++k)  
      {  
        eValue.array[k] = temp.array[k * temp.column + k];  
      }  
   
    //对特征值按照降序排序  
    SortVector(&eValue,1);  
   
    //根据特征值eValue，原始矩阵求解矩阵特征向量Q  
    Eigenvectors(&Q,&A,&eValue);  
   
    //打印特征值  
    printf("特征值：");  
    PrintMatrix(&eValue);  
   
    //打印特征向量  
    printf("特征向量");  
    PrintMatrix(&Q);  
    DestroyMatrix(&A);  
    DestroyMatrix(&R);  
    DestroyMatrix(&Q);  
    DestroyMatrix(&eValue);  
    DestroyMatrix(&temp);  
   
    return 0;  
  }  

#+END_SRC
** 矩阵求逆
#+BEGIN_SRC c -n
#include<stdio.h>  
#define N 10  
int getA(int arcs[N][N],int n)//按第一行展开计算|A|  
{  
    if(n==1)  
    {  
        return arcs[0][0];  
    }  
    int ans = 0;  
    int temp[N][N];  
    int i,j,k;  
    for(i=0;i<n;i++)  
    {  
        for(j=0;j<n-1;j++)  
        {  
            for(k=0;k<n-1;k++)  
            {  
                temp[j][k] = arcs[j+1][(k>=i)?k+1:k];  
                
            }  
        }  
        int t = getA(temp,n-1);  
        if(i%2==0)  
        {  
            ans += arcs[0][i]*t;  
        }  
        else  
        {  
            ans -=  arcs[0][i]*t;  
        }  
    }  
    return ans;  
}  
void getAStart(int arcs[N][N],int n,int ans[N][N])//计算每一行每一列的每个元素所对应的余子式，组成A*  
{  
    if(n==1)  
    {  
        ans[0][0] = 1;  
        return;  
    }  
    int i,j,k,t;  
    int temp[N][N];  
    for(i=0;i<n;i++)  
    {  
        for(j=0;j<n;j++)  
        {  
            for(k=0;k<n-1;k++)  
            {  
                for(t=0;t<n-1;t++)  
                {  
                    temp[k][t] = arcs[k>=i?k+1:k][t>=j?t+1:t];  
                }  
            }  

        
            ans[j][i]  =  getA(temp,n-1);  
            if((i+j)%2 == 1)  
            {  
                ans[j][i] = - ans[j][i];  
            }  
        }  
    }  
}  

int main()  
{  
    int arcs[N][N];  
    int astar[N][N];  
    int i,j;  
    int n;  
    while(scanf("%d",&n)!=EOF && n)  
    {  
        for(i=0;i<n;i++)  
        {  
            for(j=0;j<n;j++)  
            {  
                scanf("%d",&arcs[i][j]);  
            }  
        }  
    
        int a = getA(arcs,n);  
        if(a==0)  
        {  
            printf("can not transform!\n");  
        }  
        else  
        {  
            getAStart(arcs,n,astar);  
            for(i=0;i<n;i++)  
            {  
                for(j=0;j<n;j++)  
                {  
                    printf("%.3lf ",(double)astar[i][j]/a);  
                }  
                printf("\n");  
            }  
        }  
        printf("\n");  

    }  
    

    return 0;  
}  

#+END_SRC




* 随想
  多元时间序列有多个时间序列，之所以要使用多元原因是因为可能两只股票之间存在一定的联系，最简单的我们可以想到的是z_1t受z_2(t-1) 和z_1(t-1),也就是z1和z2前面数值的影响，受其影响的大小体现在参数φ里面，在估计过程中
我们就能将相关性大的参数估计的数大而相关性小的估计的参数小。像P21页解释的那样，要是φ1,21 ！= 0 的话说明z_2t 依赖于z_1t的过去值。这样多元时间序列的关系就体现出来了。

另外一个比较简单的模型是VMA模型。就想P79页说的那样，可以列出z_1 和 z_2t 的VMA表达式。这里面的参数是θ和∑_a 。使用的方法是极大似然算法。
在R代码里，有现成的函数nlminb来做这个极大似然函数的优化参数使这函数的object函数返回的值最小。

eVMA相比较于VMA来说假设不一样，VMA假设的是a_0 = 0 ，而eVMA假设的是a_0是一个随机变量。所以做最大似然估计的时候需要做的工作要多一些。

用什么来决定用VMA还是eVMA呢？-一般来说当样本数比较大尤其是VMA(q)可逆的时候，这两个似然估计的函数相似。如果不可逆的话选用eVMA比较合适。
模型可逆的条件在每节都有讨论。


** 一些东西短时间内看不明白，不要放弃，坚持每天看的话真的是每天都有顿悟的时刻的。
   


